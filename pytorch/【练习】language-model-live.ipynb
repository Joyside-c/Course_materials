{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(9567)\n",
    "np.random.seed(9567)\n",
    "torch.manual_seed(9567)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(9567)\n",
    "    \n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_SIZE = 100\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:10002\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(lower=True)\n",
    "train,val,test = torchtext.datasets.LanguageModelingDataset.splits \\\n",
    "(path = '..\\\\..\\\\' , train = 'text8.train.txt',validation='text8.dev.txt', \\\n",
    " test='text8.test.txt', text_field=TEXT)\n",
    "\n",
    "TEXT.build_vocab(train,max_size = MAX_VOCAB_SIZE)\n",
    "print('vocabulary size:{}'.format(len(TEXT.vocab)))\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "#构建一个iterater,torchtext.data.BPTTIterator.splits可以一起构建出来\n",
    "#使用的时候从官网上找，因为做iteration的方法还不太稳定\n",
    "#bptt_len是网络中往回传的长度有多少,即句子长度\n",
    "train_iter,val_iter,test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train,val,test),batch_size = BATCH_SIZE,device=device,bptt_len=16,\n",
    "    repeat = False,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term of abuse first used against early working class <unk> including the\n",
      "originated as a term of abuse first used against early working class <unk> including the <unk>\n"
     ]
    }
   ],
   "source": [
    "#查看一个batch，target就是预测text的下一个单词\n",
    "print(\" \" .join(TEXT.vocab.itos[i] for i in batch.text[:,0].data.cpu()))\n",
    "print(\" \" .join(TEXT.vocab.itos[i] for i in batch.target[:,0].data.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<unk> of the english revolution and the <unk> <unk> of the french revolution whilst the term\n",
      "of the english revolution and the <unk> <unk> of the french revolution whilst the term is\n",
      "1\n",
      "is still used in a <unk> way to describe any act that used violent means to\n",
      "still used in a <unk> way to describe any act that used violent means to destroy\n",
      "2\n",
      "destroy the organization of society it has also been taken up as a positive label by\n",
      "the organization of society it has also been taken up as a positive label by self\n",
      "3\n",
      "self defined anarchists the word anarchism is derived from the greek without <unk> ruler chief king\n",
      "defined anarchists the word anarchism is derived from the greek without <unk> ruler chief king anarchism\n",
      "4\n",
      "anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished\n",
      "as a political philosophy is the belief that rulers are unnecessary and should be abolished although\n"
     ]
    }
   ],
   "source": [
    "#多查看 几个batch\n",
    "#可见单词都是连续预测出来的\n",
    "for i in range(5):\n",
    "    batch = next(it)\n",
    "    print(i)\n",
    "    print(\" \" .join(TEXT.vocab.itos[i] for i in batch.text[:,0].data.cpu()))\n",
    "    print(\" \" .join(TEXT.vocab.itos[i] for i in batch.target[:,0].data.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNNModel(nn.Module):\n",
    "    #初始化模型，想：我们想要拿到什么东西\n",
    "    def __init__(self,  vocab_size,embed_size,hidden_size):\n",
    "        super(RNNModel,self).__init__()\n",
    "        #embed层，一进来一句话先embed成128维的向量\n",
    "        self.encoder = nn.Embedding(vocab_size,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size)\n",
    "        #为了把最后一层10002维的向量argmax成一句话\n",
    "        self.linear = nn.Linear(hidden_size,vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    \n",
    "    def forward(self, text, hidden):\n",
    "        #forward pass\n",
    "        #首先要embedding\n",
    "        #text:seq_length * batch_size\n",
    "        #torch做这些处理的时候默认\n",
    "        #第一维是seq_length的，第二维是batch_size的\n",
    "        emb = self.encoder(text)#seq_length*batch_size*embed_size\n",
    "        #直接把embeding的结果传进RNN\n",
    "        #output:seq_len*batch_size* hidden_size\n",
    "        #hidden:(1*batch_size* hidden_size,1*batch_size* hidden_size)\n",
    "        output,hidden = self.lstm(emb,hidden)\n",
    "        #把output的前两个维度拼到一起\n",
    "        #(seq_len*batch_size)* hidden_size\n",
    "        out_vocab = self.linear(output.view(-1,output.shape[2]))#(seq_len*batch_size)* vocab_size\n",
    "        out_vocab = out_vocab.view(output.size(0),output.size(1),out_vocab.size(-1))\n",
    "        #想要知道每一个位置分别预测的哪个单词\n",
    "        return out_vocab, hidden\n",
    "    \n",
    "    def init_hidden(self,bsz,requires_grad = True):\n",
    "        weight = next(self.parameters())\n",
    "        #因为给LSTM,所以要返回两个state\n",
    "        return (weight.new_zeros((1,bsz,self.hidden_size),requires_grad=True),\n",
    "                weight.new_zeros((1,bsz,self.hidden_size),requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size=len(TEXT.vocab),\n",
    "                                embed_size=EMBEDDING_SIZE,\n",
    "                                hidden_size = HIDDEN_SIZE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.3284, -2.5221, -0.7626,  ...,  1.2552,  0.3371,  0.8519],\n",
       "        [ 1.0139, -1.3351,  1.0443,  ...,  0.8238,  0.8505, -0.2647],\n",
       "        [-0.8093,  2.1890,  0.2798,  ...,  0.3279, -0.3946,  1.0357],\n",
       "        ...,\n",
       "        [ 0.1942,  0.6266, -1.0434,  ..., -1.2402,  1.2847, -0.6255],\n",
       "        [ 0.0097,  1.6791, -0.4318,  ..., -1.1566,  0.9146,  0.3966],\n",
       "        [ 1.0256, -1.3016, -0.7302,  ...,  0.0186, -1.7787, -0.1317]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h,torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    #相当于复制了一个只保存值不保存历史的tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "#所有参数传进去优化\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,data):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    totoal_count = 0.\n",
    "    it = iter(data)\n",
    "    with torch.no_grad():\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for i ,batch in enumerate(it):\n",
    "            data,target = batch.text,batch.target\n",
    "            #把隐层一直往下传？\n",
    "            #backprorgate through all iterations 内存很快会爆掉 只有语言模型需要这样做\n",
    "            hidden = repackage_hidden(hidden)#通过repackage确保这是一个全新的hidden，不带历史信息\n",
    "            output,hidden = model(data,hidden)\n",
    "\n",
    "            loss = loss_fn(output.view(-1,VOCAB_SIZE),target.view(-1))#batch_size* target_class_dim,batch_size\n",
    "            #因为loss是被平均过的，所以\n",
    "            total_loss = loss.item()*np.multiply(*data.size())\n",
    "            total_count = np.multiply(*data.size())\n",
    "    loss = total_loss\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 9.199448585510254\n",
      "loss 9.20296573638916\n",
      "loss 9.202123641967773\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-ece0760f332c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#batch_size* target_class_dim,batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 932\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1533\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1535\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1536\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1537\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS=2\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "GRAD_CLIP = 5.\n",
    "#val_lossoes = \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    #torch里的模型有两种，一种是训练模式，一种是测试模式\n",
    "    #两个模式有很多东西是不一样的\n",
    "    model.train()\n",
    "    it = iter(train_iter)\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for i ,batch in enumerate(it):\n",
    "        data,target = batch.text,batch.target\n",
    "        #把隐层一直往下传？\n",
    "        #backprorgate through all iterations 内存很快会爆掉 只有语言模型需要这样做\n",
    "        hidden = repackage_hidden(hidden)#通过repackage确保这是一个全新的hidden，不带历史信息\n",
    "        output,hidden = model(data,hidden)\n",
    "        \n",
    "        loss = loss_fn(output.view(-1,VOCAB_SIZE),target.view(-1))#batch_size* target_class_dim,batch_size\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if  i %100 ==0:\n",
    "            print('loss',loss.item())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load模型的方法\n",
    "#首先建一个模型\n",
    "best_model = RNNModel(vocab_size=len(TEXT.vocab),\n",
    "                                embed_size=EMBEDDING_SIZE,\n",
    "                                hidden_size = HIDDEN_SIZE)\n",
    "best_model.load_state_dixt(torch.load('lm.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
